{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-03T22:06:27.543155Z","iopub.execute_input":"2022-09-03T22:06:27.543580Z","iopub.status.idle":"2022-09-03T22:06:27.559435Z","shell.execute_reply.started":"2022-09-03T22:06:27.543542Z","shell.execute_reply":"2022-09-03T22:06:27.557811Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/amex-default-prediction/sample_submission.csv\n/kaggle/input/amex-default-prediction/train_data.csv\n/kaggle/input/amex-default-prediction/test_data.csv\n/kaggle/input/amex-default-prediction/train_labels.csv\n/kaggle/input/amex-train-pq/ex.parquet\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Library Imports\n","metadata":{}},{"cell_type":"code","source":"import gc\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.csv as pv\nimport pyarrow.parquet as pq\nimport multiprocessing as mp\n# from tqdm.auto import tqdm\n# import itertools\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 1000)","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:06:29.187818Z","iopub.execute_input":"2022-09-03T22:06:29.188234Z","iopub.status.idle":"2022-09-03T22:06:29.339524Z","shell.execute_reply.started":"2022-09-03T22:06:29.188202Z","shell.execute_reply":"2022-09-03T22:06:29.338173Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## The enviornment we are running this in is memory constrained.\nTo combat this, we will be doing a few things. Ultimately, we desire to reduce memory costs.\n1. Iterate through file using a cursor\n2. Convert file from CSV to Parquet\n3. Check for and Reformat improperly stored data\n  * Unique integers saved as strings, etc. \n  \n*Iterate through file using a cursor*<br>\nIterating through the file, while more memory stable, does produce some inherit costs. <br>\n1. open_csv() is the only *supported* way to instantiate a CSVStreamingReader in pyarrow.\n2. open_csv() does not support multiprocessing, even when setting multiprocessing to true.\n3. Setting a read_size in pyarrow will only change the chunking size. The open file may continue to read into memory, reducing resources for other processing.\n**It may not enough to set read_size, CSV may need to be opened in a preallocated memory pool.**<br>\nHowever CPU isn't an issue. So at this time, we will open both CSVStreamingReader and ParquetWriter simultaneously in a spawned process.","metadata":{}},{"cell_type":"code","source":"import resource\ndef mem():\n    print('Memory usage         : % 2.2f MB' % round(\n        resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024.0,1))","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:06:31.884815Z","iopub.execute_input":"2022-09-03T22:06:31.885752Z","iopub.status.idle":"2022-09-03T22:06:31.890643Z","shell.execute_reply.started":"2022-09-03T22:06:31.885713Z","shell.execute_reply":"2022-09-03T22:06:31.889859Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Note: While pyarrow.csv.StreamingReader documentation says it has a close() method, it does not. MUST be opened by with statement\ndef convert_csv_to_parquet(origin_path, destination_path, pv_read_options):\n    with pv.open_csv('../input/amex-default-prediction/train_data.csv', read_options=pv_read_options) as csv_cursor:\n        with pq.ParquetWriter('ex.parquet', csv_cursor.schema) as writer:\n            while True:\n                try:\n                    batch = csv_cursor.read_next_batch()\n                    writer.write_batch(batch)\n                except StopIteration:\n                    print('Reached End')\n                    mem()\n                    batch = None\n                    gc.collect()\n                    break\n                batch = None\n                gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_path = '../input/amex-default-prediction/train_data.csv'\npv_read_options = pv.ReadOptions(block_size=268435456)\nparquet_dest = 'ex.parquet'\n\nmem()\nconvert_process = mp.Process(target=convert_csv_to_parquet, args=(csv_path, parquet_dest, pv_read_options))\nconvert_process.start()\nconvert_process.join()\nmem()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reduction in File Size\nHere we see, the 16.39GB CSV file has been reduced to a 6.97GB Parquet file. <br>\nEven in our memory constrained environment, we could read all of this to memory, so long as we clean up after ourselves.\n\n## Limitations of Garbage Collection\nWhile we have called gc.collect() several times, it is important to note; the gc module doesn't track all the files we created. The blinding large set of data is creating a 'free list'. This will pose an issue when we need to free up memory. There are several methods possible for garbage collection, however without a reference to the objects created, gc.collect() will leave the data in memory. When pyarrow reads in the data, it does so to the Arrow buffer. Even when dealing with the Arrow buffer directly, it is not guaranteed that the memory will be returned to the OS, this is a conundrum of large file sizes. In this scenario, given the formats the data can be read into memory and the need to preserve the resources for ensemble calculation, the function will have to occur in a subprocess to most efficiently return the memory.","metadata":{}},{"cell_type":"code","source":"def create_array():\n    dataset = pq.ParquetDataset('../input/amex-train-pq/ex.parquet')\n    pq_array = dataset.read()\n    mem()\nmem()\np = mp.Process(target=create_array, args=())\np.start()\np.join()\nmem()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spawning Processes Returns Memory upon join()\nWhile there are limitations in spawing processes to handle memory-hogging tasks, using means outside of the pyarrow library to multiprocess a document often requires Cython a boilerplate. Given the limitations of the interface, we aren't sacrificing much.<br>\n## However, we should still be only accessing one column at a time. <h4>We will need to do two forms of iterative loading:</h4>\n* Row Based\n  * This will be the base access for ensamble algorithms\n  * At current; ParquetDataset doesn't support split_row_groups, or any other form of row seperation/filtering, however ParquetFile module supports batch reading.\n      * We will need to change the format we work with to a pyarrow Array in order to allow quicker processing and translation between endpoints.\n* Column Based\n  * This will be the base access for Feature Creation </ul></ul>\nIt is important to remember the shape of our table has Features in Columns, and Persons in rows. <br>\nIterative loading has several benefits, and with proper feature creation and tracking, would be the proper access of a dataset this large even outside of the constrains of the environment.\n\npyarrow has a very powerful API for ","metadata":{}},{"cell_type":"code","source":"pq_path = '../input/amex-train-pq/ex.parquet'\ndataset = pq.ParquetDataset(pq_path, use_legacy_dataset=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:06:35.434222Z","iopub.execute_input":"2022-09-03T22:06:35.434597Z","iopub.status.idle":"2022-09-03T22:06:35.572310Z","shell.execute_reply.started":"2022-09-03T22:06:35.434566Z","shell.execute_reply":"2022-09-03T22:06:35.571286Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Convert Customer ID\n<h4>customer_ID</h4> \ncustomer_ID is a string, if we take a subsection of customer ID and redefine it as a integer we save a great deal of space. This requires that we verify that all our base16 strings are converted to base10 without overlapping. We will be converting them uniformly, and checking the number of uniques before and after transformation. If these numbers are the same, we can infer that the transfer was successful. ","metadata":{}},{"cell_type":"code","source":"import sys\nmem()\ndef convert_cust_id(dataset):\n    schema = dataset.schema\n    table = dataset.read(['customer_ID'])\n    mem()\n    df = table.to_pandas()\n    display(df.customer_ID.nunique())\n    print(sys.getsizeof(df.customer_ID))\n    df.customer_ID = df.customer_ID.apply(lambda x: int(x[-16:], base=16)).astype('int64')\n    display(df.customer_ID.nunique())\n    print(sys.getsizeof(df.customer_ID))\n#     We see we've reduced the ~669MB memory of the df.customer_ID object to ~44MB\n#     We used a apply(lambda) to do this to each row and we are returned with the same number of unique items, indicating a successful transfer to base10\n\np = mp.Process(target=convert_cust_id, args=(dataset,))\np.start()\np.join()\nmem()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:39:41.525349Z","iopub.execute_input":"2022-09-02T19:39:41.525766Z","iopub.status.idle":"2022-09-02T19:39:48.890218Z","shell.execute_reply.started":"2022-09-02T19:39:41.525726Z","shell.execute_reply":"2022-09-02T19:39:48.888807Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Memory usage         :  256.70 MB\nMemory usage         :  636.60 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"458913"},"metadata":{}},{"name":"stdout","text":"669305731\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"458913"},"metadata":{}},{"name":"stdout","text":"44251768\nMemory usage         :  256.70 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Other Datatype Conversions","metadata":{}},{"cell_type":"markdown","source":"## Review Data Row by Row\nWe've iterated through our parquet columns and checked for unique counts, we've found the following  columns boil down to 8 unique values maximum. <br>\nKnowing this we are able to recategorize these categoricals as integers.<br>\n['D_63', 'D_64', 'D_66', 'D_68', 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126'] <br>\nSo here we need to:\n*     create a function that maps the unique variables of the column to a integer of base=8\n*     apply that function to every column\n*     convert back to parquet\n*     observe size changes","metadata":{}},{"cell_type":"code","source":"# Convert date string to datetime object\n#     table = dataset.read(['D_63', 'D_64', 'D_66', 'D_68', 'B_30', B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126'])\n\nimport sys\nmem()\ndef review_data(dataset):\n    schema = dataset.schema\n    n = len(schema)\n    for i in range(n):\n        col = schema[i].name\n        table = dataset.read([col])\n        print(table.group_by('col').aggregate([(\"count_distinct\")]))\n        nunique = pa.compute.count_distinct(table[col])\n        pd_nunique = table.to_pandas().nunique()\n    cols = ['D_63', 'D_64', 'D_66', 'D_68', 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126']\n    table = dataset.read(cols)\n    df = table.to_pandas()\n    for i in cols:\n        print(df[i].value_counts())\n        df[i] = df[i].apply(lambda x: int(x, base=8)).astype('int8')\n    display(df)\n    df.info()\n\ndef int_map(df):\n    for i in df.columns:\n        n = df[i].nunique()\n        for num in n:\n            pass\n\np = mp.Process(target=review_data, args=(dataset,))\np.start()\np.join()\nmem()","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:09:05.441465Z","iopub.execute_input":"2022-09-03T22:09:05.441941Z","iopub.status.idle":"2022-09-03T22:09:07.447026Z","shell.execute_reply.started":"2022-09-03T22:09:05.441882Z","shell.execute_reply":"2022-09-03T22:09:07.445670Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Memory usage         :  254.40 MB\nCO    4119621\nCR     930133\nCL     438390\nXZ      25786\nXM      10556\nXL       6965\nName: D_63, dtype: int64\nO     2913244\nU     1523448\nR      840112\n       217442\n-1      37205\nName: D_64, dtype: int64\n1.0    617066\n0.0      6288\nName: D_66, dtype: int64\n6.0    2782455\n5.0    1201706\n3.0     484442\n4.0     477187\n2.0     220111\n1.0     133122\n0.0      15925\nName: D_68, dtype: int64\n0.0    4710663\n1.0     763955\n2.0      54817\nName: B_30, dtype: int64\n2.0    1953232\n3.0    1255315\n1.0    1160047\n5.0     444856\n4.0     294917\n7.0     259028\n6.0     162040\nName: B_38, dtype: int64\n1.0    3316478\n0.0    2038257\nName: D_114, dtype: int64\n0.0    5348109\n1.0       6626\nName: D_116, dtype: int64\n-1.0    1456084\n 3.0    1166400\n 4.0    1138666\n 2.0     666808\n 5.0     459290\n 6.0     344520\n 1.0     122967\nName: D_117, dtype: int64\n0.0    4729723\n1.0     625012\nName: D_120, dtype: int64\n 1.0    4262414\n 0.0     891323\n-1.0     260898\nName: D_126, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"        D_63 D_64  D_66  D_68  B_30  B_38  D_114  D_116  D_117  D_120  D_126\n0         CR    O   NaN   6.0   0.0   2.0    1.0    0.0    4.0    0.0    1.0\n1         CR    O   NaN   6.0   0.0   2.0    1.0    0.0    4.0    0.0    1.0\n2         CR    O   NaN   6.0   0.0   2.0    1.0    0.0    4.0    0.0    1.0\n3         CR    O   NaN   6.0   0.0   2.0    1.0    0.0    4.0    0.0    1.0\n4         CR    O   NaN   6.0   0.0   2.0    1.0    0.0    4.0    0.0    1.0\n...      ...  ...   ...   ...   ...   ...    ...    ...    ...    ...    ...\n5531446   CL    O   NaN   5.0   0.0   3.0    1.0    0.0    3.0    0.0    1.0\n5531447   CL    O   NaN   5.0   0.0   3.0    1.0    0.0    3.0    0.0    1.0\n5531448   CL    O   NaN   5.0   0.0   3.0    1.0    0.0    3.0    0.0    1.0\n5531449   CL    O   NaN   5.0   0.0   3.0    1.0    0.0    3.0    0.0    1.0\n5531450   CL    O   NaN   5.0   0.0   3.0    1.0    0.0    3.0    0.0    1.0\n\n[5531451 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>D_63</th>\n      <th>D_64</th>\n      <th>D_66</th>\n      <th>D_68</th>\n      <th>B_30</th>\n      <th>B_38</th>\n      <th>D_114</th>\n      <th>D_116</th>\n      <th>D_117</th>\n      <th>D_120</th>\n      <th>D_126</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CR</td>\n      <td>O</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CR</td>\n      <td>O</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CR</td>\n      <td>O</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CR</td>\n      <td>O</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CR</td>\n      <td>O</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5531446</th>\n      <td>CL</td>\n      <td>O</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5531447</th>\n      <td>CL</td>\n      <td>O</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5531448</th>\n      <td>CL</td>\n      <td>O</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5531449</th>\n      <td>CL</td>\n      <td>O</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5531450</th>\n      <td>CL</td>\n      <td>O</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5531451 rows × 11 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5531451 entries, 0 to 5531450\nData columns (total 11 columns):\n #   Column  Dtype  \n---  ------  -----  \n 0   D_63    object \n 1   D_64    object \n 2   D_66    float64\n 3   D_68    float64\n 4   B_30    float64\n 5   B_38    float64\n 6   D_114   float64\n 7   D_116   float64\n 8   D_117   float64\n 9   D_120   float64\n 10  D_126   float64\ndtypes: float64(9), object(2)\nmemory usage: 464.2+ MB\nMemory usage         :  254.40 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"mem()\ndef columnar_scan(dataset):\n    schema = dataset.schema\n    for i in schema:\n        table = dataset.read([i.name])\n        break\n    mem()\n    df = table.to_pandas()\n    customer_ID_group = df.groupby(['customer_ID'])\n    df = customer_ID_group['customer_ID'].count().reset_index(name=\"count\")\n    display(df.query('count < 13'))\ndef row_scan(dataset):\n    pass\n\np = mp.Process(target=columnar_scan, args=(dataset,))\np.start()\np.join()\nmem()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:51:43.831972Z","iopub.execute_input":"2022-08-31T17:51:43.832479Z","iopub.status.idle":"2022-08-31T17:51:46.507880Z","shell.execute_reply.started":"2022-08-31T17:51:43.832434Z","shell.execute_reply":"2022-08-31T17:51:46.505897Z"},"trusted":true},"execution_count":168,"outputs":[{"name":"stdout","text":"Memory usage         :  298.70 MB\nMemory usage         :  677.80 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                              customer_ID  count\n10      0001337ded4e1c2539d1a78ff44a457bd4a95caa55ba17...      3\n20      000391f219520dbca6c3c1c46e0fab569da163f79ee266...      4\n27      0004860c260168fcaad0734a1dfedb7ceb1a83aaac54e2...      9\n35      00057c2d8d887fa3f777d97dc939700731575772e6c990...      4\n36      0005a6ae24fd274640a237ea56c43b1ef9e32077ad168a...     12\n...                                                   ...    ...\n458895  fffe13e28dc3ceadf28249b596ba25df93e38ec53d38cf...      3\n458896  fffe2bc02423407e33a607660caeed076d713d8a5ad323...      8\n458899  fffe5008118592b867d89647fc840c45858860f596d98b...      2\n458903  fffec7d7e1ca804c86f1ffdaac389c33f8039ed35bf412...      7\n458904  fffee056e120fb326c9413fca5a7ab6618cc49be9bb6b1...      2\n\n[72879 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>0001337ded4e1c2539d1a78ff44a457bd4a95caa55ba17...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>000391f219520dbca6c3c1c46e0fab569da163f79ee266...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0004860c260168fcaad0734a1dfedb7ceb1a83aaac54e2...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>00057c2d8d887fa3f777d97dc939700731575772e6c990...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0005a6ae24fd274640a237ea56c43b1ef9e32077ad168a...</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>458895</th>\n      <td>fffe13e28dc3ceadf28249b596ba25df93e38ec53d38cf...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>458896</th>\n      <td>fffe2bc02423407e33a607660caeed076d713d8a5ad323...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>458899</th>\n      <td>fffe5008118592b867d89647fc840c45858860f596d98b...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>458903</th>\n      <td>fffec7d7e1ca804c86f1ffdaac389c33f8039ed35bf412...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>458904</th>\n      <td>fffee056e120fb326c9413fca5a7ab6618cc49be9bb6b1...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>72879 rows × 2 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"Memory usage         :  298.70 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"mem()\ndef row_scan(path):\n    pf = pq.ParquetFile(path)\n    row = next(pf.iter_batches(batch_size = 1))\n    df = pa.Table.from_batches([row]).to_pandas()\n    mem()\n    display(df)\n    print(df.customer_ID[0])\n    \n    \n\np = mp.Process(target=row_scan, args=(pq_path,))\np.start()\np.join()\nmem()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T16:25:48.667244Z","iopub.execute_input":"2022-08-31T16:25:48.667732Z","iopub.status.idle":"2022-08-31T16:25:49.097983Z","shell.execute_reply.started":"2022-08-31T16:25:48.667692Z","shell.execute_reply":"2022-08-31T16:25:49.096843Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"Memory usage         :  298.70 MB\nMemory usage         :  598.10 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                         customer_ID         S_2       P_2      D_39       B_1       B_2       R_1       S_3      D_41       B_3  D_42  D_43     D_44       B_4      D_45     B_5       R_2      D_46      D_47      D_48  D_49       B_6       B_7       B_8      D_50      D_51       B_9       R_3      D_52       P_3      B_10  D_53       S_5      B_11       S_6      D_54       R_4       S_7      B_12       S_8      D_55      D_56      B_13       R_5      D_58       S_9      B_14      D_59      D_60      D_61      B_15      S_11      D_62 D_63 D_64      D_65      B_16  B_17      B_18     B_19  D_66     B_20  D_68      S_12       R_6      S_13      B_21      D_69      B_22      D_70      D_71      D_72      S_15      B_23  D_73       P_4      D_74      D_75  D_76      B_24       R_7  D_77      B_25      B_26      D_78      D_79       R_8  R_9      S_16      D_80      R_10      R_11     B_27      D_81      D_82      S_17      R_12      B_28     R_13      D_83      R_14  \\\n0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-03-09  0.938469  0.001733  0.008724  1.006838  0.009228  0.124035  0.008771  0.004709   NaN   NaN  0.00063  0.080986  0.708906  0.1706  0.006204  0.358587  0.525351  0.255736   NaN  0.063902  0.059416  0.006466  0.148698  1.335856  0.008207  0.001423  0.207334  0.736463  0.096219   NaN  0.023381  0.002768  0.008322  1.001519  0.008298  0.161345  0.148266  0.922998  0.354596  0.152025  0.118075  0.001882  0.158612  0.065728  0.018385  0.063646  0.199617  0.308233  0.016361  0.401619  0.091071   CR    O  0.007126  0.007665   NaN  0.652984  0.00852   NaN  0.00473   6.0  0.272008  0.008363  0.515222  0.002644  0.009013  0.004808  0.008342  0.119403  0.004802  0.108271  0.050882   NaN  0.007554  0.080422  0.069067   NaN  0.004327  0.007562   NaN  0.007729  0.000272  0.001576  0.004239  0.001434  NaN  0.002271  0.004061  0.007121  0.002456  0.00231  0.003532  0.506612  0.008033  1.009825  0.084683  0.00382  0.007043  0.000438   \n\n       R_15     D_84      R_16  B_29  B_30     S_18      D_86  D_87      R_17      R_18  D_88  B_31      S_19      R_19      B_32      S_20      R_20     R_21      B_33      D_89      R_22      R_23      D_91      D_92      D_93      D_94     R_24      R_25     D_96     S_22      S_23      S_24      S_25      S_26     D_102     D_103     D_104     D_105  D_106     D_107      B_36      B_37  R_26      R_27  B_38  D_108     D_109  D_110  D_111  B_39     D_112     B_40      S_27     D_113  D_114    D_115  D_116  D_117    D_118     D_119  D_120    D_121     D_122     D_123     D_124    D_125  D_126     D_127     D_128    D_129      B_41  B_42     D_130     D_131  D_132     D_133      R_28  D_134  D_135  D_136  D_137  D_138     D_139     D_140     D_141  D_142     D_143    D_144     D_145  \n0  0.006452  0.00083  0.005055   NaN   0.0  0.00572  0.007084   NaN  0.000198  0.008907   NaN     1  0.002537  0.005177  0.006626  0.009705  0.007782  0.00245  1.001101  0.002665  0.007479  0.006893  1.503673  1.006133  0.003569  0.008871  0.00395  0.003647  0.00495  0.89409  0.135561  0.911191  0.974539  0.001243  0.766688  1.008691  1.004587  0.893734    NaN  0.670041  0.009968  0.004572   NaN  1.008949   2.0    NaN  0.004326    NaN    NaN   NaN  1.007336  0.21006  0.676922  0.007871    1.0  0.23825    0.0    4.0  0.23212  0.236266    0.0  0.70228  0.434345  0.003057  0.686516  0.00874    1.0  1.003319  1.007819  1.00008  0.006805   NaN  0.002052  0.005972    NaN  0.004345  0.001535    NaN    NaN    NaN    NaN    NaN  0.002427  0.003706  0.003818    NaN  0.000569  0.00061  0.002674  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>S_2</th>\n      <th>P_2</th>\n      <th>D_39</th>\n      <th>B_1</th>\n      <th>B_2</th>\n      <th>R_1</th>\n      <th>S_3</th>\n      <th>D_41</th>\n      <th>B_3</th>\n      <th>D_42</th>\n      <th>D_43</th>\n      <th>D_44</th>\n      <th>B_4</th>\n      <th>D_45</th>\n      <th>B_5</th>\n      <th>R_2</th>\n      <th>D_46</th>\n      <th>D_47</th>\n      <th>D_48</th>\n      <th>D_49</th>\n      <th>B_6</th>\n      <th>B_7</th>\n      <th>B_8</th>\n      <th>D_50</th>\n      <th>D_51</th>\n      <th>B_9</th>\n      <th>R_3</th>\n      <th>D_52</th>\n      <th>P_3</th>\n      <th>B_10</th>\n      <th>D_53</th>\n      <th>S_5</th>\n      <th>B_11</th>\n      <th>S_6</th>\n      <th>D_54</th>\n      <th>R_4</th>\n      <th>S_7</th>\n      <th>B_12</th>\n      <th>S_8</th>\n      <th>D_55</th>\n      <th>D_56</th>\n      <th>B_13</th>\n      <th>R_5</th>\n      <th>D_58</th>\n      <th>S_9</th>\n      <th>B_14</th>\n      <th>D_59</th>\n      <th>D_60</th>\n      <th>D_61</th>\n      <th>B_15</th>\n      <th>S_11</th>\n      <th>D_62</th>\n      <th>D_63</th>\n      <th>D_64</th>\n      <th>D_65</th>\n      <th>B_16</th>\n      <th>B_17</th>\n      <th>B_18</th>\n      <th>B_19</th>\n      <th>D_66</th>\n      <th>B_20</th>\n      <th>D_68</th>\n      <th>S_12</th>\n      <th>R_6</th>\n      <th>S_13</th>\n      <th>B_21</th>\n      <th>D_69</th>\n      <th>B_22</th>\n      <th>D_70</th>\n      <th>D_71</th>\n      <th>D_72</th>\n      <th>S_15</th>\n      <th>B_23</th>\n      <th>D_73</th>\n      <th>P_4</th>\n      <th>D_74</th>\n      <th>D_75</th>\n      <th>D_76</th>\n      <th>B_24</th>\n      <th>R_7</th>\n      <th>D_77</th>\n      <th>B_25</th>\n      <th>B_26</th>\n      <th>D_78</th>\n      <th>D_79</th>\n      <th>R_8</th>\n      <th>R_9</th>\n      <th>S_16</th>\n      <th>D_80</th>\n      <th>R_10</th>\n      <th>R_11</th>\n      <th>B_27</th>\n      <th>D_81</th>\n      <th>D_82</th>\n      <th>S_17</th>\n      <th>R_12</th>\n      <th>B_28</th>\n      <th>R_13</th>\n      <th>D_83</th>\n      <th>R_14</th>\n      <th>R_15</th>\n      <th>D_84</th>\n      <th>R_16</th>\n      <th>B_29</th>\n      <th>B_30</th>\n      <th>S_18</th>\n      <th>D_86</th>\n      <th>D_87</th>\n      <th>R_17</th>\n      <th>R_18</th>\n      <th>D_88</th>\n      <th>B_31</th>\n      <th>S_19</th>\n      <th>R_19</th>\n      <th>B_32</th>\n      <th>S_20</th>\n      <th>R_20</th>\n      <th>R_21</th>\n      <th>B_33</th>\n      <th>D_89</th>\n      <th>R_22</th>\n      <th>R_23</th>\n      <th>D_91</th>\n      <th>D_92</th>\n      <th>D_93</th>\n      <th>D_94</th>\n      <th>R_24</th>\n      <th>R_25</th>\n      <th>D_96</th>\n      <th>S_22</th>\n      <th>S_23</th>\n      <th>S_24</th>\n      <th>S_25</th>\n      <th>S_26</th>\n      <th>D_102</th>\n      <th>D_103</th>\n      <th>D_104</th>\n      <th>D_105</th>\n      <th>D_106</th>\n      <th>D_107</th>\n      <th>B_36</th>\n      <th>B_37</th>\n      <th>R_26</th>\n      <th>R_27</th>\n      <th>B_38</th>\n      <th>D_108</th>\n      <th>D_109</th>\n      <th>D_110</th>\n      <th>D_111</th>\n      <th>B_39</th>\n      <th>D_112</th>\n      <th>B_40</th>\n      <th>S_27</th>\n      <th>D_113</th>\n      <th>D_114</th>\n      <th>D_115</th>\n      <th>D_116</th>\n      <th>D_117</th>\n      <th>D_118</th>\n      <th>D_119</th>\n      <th>D_120</th>\n      <th>D_121</th>\n      <th>D_122</th>\n      <th>D_123</th>\n      <th>D_124</th>\n      <th>D_125</th>\n      <th>D_126</th>\n      <th>D_127</th>\n      <th>D_128</th>\n      <th>D_129</th>\n      <th>B_41</th>\n      <th>B_42</th>\n      <th>D_130</th>\n      <th>D_131</th>\n      <th>D_132</th>\n      <th>D_133</th>\n      <th>R_28</th>\n      <th>D_134</th>\n      <th>D_135</th>\n      <th>D_136</th>\n      <th>D_137</th>\n      <th>D_138</th>\n      <th>D_139</th>\n      <th>D_140</th>\n      <th>D_141</th>\n      <th>D_142</th>\n      <th>D_143</th>\n      <th>D_144</th>\n      <th>D_145</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n      <td>2017-03-09</td>\n      <td>0.938469</td>\n      <td>0.001733</td>\n      <td>0.008724</td>\n      <td>1.006838</td>\n      <td>0.009228</td>\n      <td>0.124035</td>\n      <td>0.008771</td>\n      <td>0.004709</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00063</td>\n      <td>0.080986</td>\n      <td>0.708906</td>\n      <td>0.1706</td>\n      <td>0.006204</td>\n      <td>0.358587</td>\n      <td>0.525351</td>\n      <td>0.255736</td>\n      <td>NaN</td>\n      <td>0.063902</td>\n      <td>0.059416</td>\n      <td>0.006466</td>\n      <td>0.148698</td>\n      <td>1.335856</td>\n      <td>0.008207</td>\n      <td>0.001423</td>\n      <td>0.207334</td>\n      <td>0.736463</td>\n      <td>0.096219</td>\n      <td>NaN</td>\n      <td>0.023381</td>\n      <td>0.002768</td>\n      <td>0.008322</td>\n      <td>1.001519</td>\n      <td>0.008298</td>\n      <td>0.161345</td>\n      <td>0.148266</td>\n      <td>0.922998</td>\n      <td>0.354596</td>\n      <td>0.152025</td>\n      <td>0.118075</td>\n      <td>0.001882</td>\n      <td>0.158612</td>\n      <td>0.065728</td>\n      <td>0.018385</td>\n      <td>0.063646</td>\n      <td>0.199617</td>\n      <td>0.308233</td>\n      <td>0.016361</td>\n      <td>0.401619</td>\n      <td>0.091071</td>\n      <td>CR</td>\n      <td>O</td>\n      <td>0.007126</td>\n      <td>0.007665</td>\n      <td>NaN</td>\n      <td>0.652984</td>\n      <td>0.00852</td>\n      <td>NaN</td>\n      <td>0.00473</td>\n      <td>6.0</td>\n      <td>0.272008</td>\n      <td>0.008363</td>\n      <td>0.515222</td>\n      <td>0.002644</td>\n      <td>0.009013</td>\n      <td>0.004808</td>\n      <td>0.008342</td>\n      <td>0.119403</td>\n      <td>0.004802</td>\n      <td>0.108271</td>\n      <td>0.050882</td>\n      <td>NaN</td>\n      <td>0.007554</td>\n      <td>0.080422</td>\n      <td>0.069067</td>\n      <td>NaN</td>\n      <td>0.004327</td>\n      <td>0.007562</td>\n      <td>NaN</td>\n      <td>0.007729</td>\n      <td>0.000272</td>\n      <td>0.001576</td>\n      <td>0.004239</td>\n      <td>0.001434</td>\n      <td>NaN</td>\n      <td>0.002271</td>\n      <td>0.004061</td>\n      <td>0.007121</td>\n      <td>0.002456</td>\n      <td>0.00231</td>\n      <td>0.003532</td>\n      <td>0.506612</td>\n      <td>0.008033</td>\n      <td>1.009825</td>\n      <td>0.084683</td>\n      <td>0.00382</td>\n      <td>0.007043</td>\n      <td>0.000438</td>\n      <td>0.006452</td>\n      <td>0.00083</td>\n      <td>0.005055</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.00572</td>\n      <td>0.007084</td>\n      <td>NaN</td>\n      <td>0.000198</td>\n      <td>0.008907</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0.002537</td>\n      <td>0.005177</td>\n      <td>0.006626</td>\n      <td>0.009705</td>\n      <td>0.007782</td>\n      <td>0.00245</td>\n      <td>1.001101</td>\n      <td>0.002665</td>\n      <td>0.007479</td>\n      <td>0.006893</td>\n      <td>1.503673</td>\n      <td>1.006133</td>\n      <td>0.003569</td>\n      <td>0.008871</td>\n      <td>0.00395</td>\n      <td>0.003647</td>\n      <td>0.00495</td>\n      <td>0.89409</td>\n      <td>0.135561</td>\n      <td>0.911191</td>\n      <td>0.974539</td>\n      <td>0.001243</td>\n      <td>0.766688</td>\n      <td>1.008691</td>\n      <td>1.004587</td>\n      <td>0.893734</td>\n      <td>NaN</td>\n      <td>0.670041</td>\n      <td>0.009968</td>\n      <td>0.004572</td>\n      <td>NaN</td>\n      <td>1.008949</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>0.004326</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.007336</td>\n      <td>0.21006</td>\n      <td>0.676922</td>\n      <td>0.007871</td>\n      <td>1.0</td>\n      <td>0.23825</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.23212</td>\n      <td>0.236266</td>\n      <td>0.0</td>\n      <td>0.70228</td>\n      <td>0.434345</td>\n      <td>0.003057</td>\n      <td>0.686516</td>\n      <td>0.00874</td>\n      <td>1.0</td>\n      <td>1.003319</td>\n      <td>1.007819</td>\n      <td>1.00008</td>\n      <td>0.006805</td>\n      <td>NaN</td>\n      <td>0.002052</td>\n      <td>0.005972</td>\n      <td>NaN</td>\n      <td>0.004345</td>\n      <td>0.001535</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002427</td>\n      <td>0.003706</td>\n      <td>0.003818</td>\n      <td>NaN</td>\n      <td>0.000569</td>\n      <td>0.00061</td>\n      <td>0.002674</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a\nMemory usage         :  298.70 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"todo:\n* Change handling to pyarrow arrays\n* Change processing actions from pandas to pyarrow\n","metadata":{}}]}